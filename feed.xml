<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jeffrey Maassen</title>
    <description>A master student in the deparment of Computer Science, Stevens Institute of Technology.
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 31 May 2020 20:02:40 -0400</pubDate>
    <lastBuildDate>Sun, 31 May 2020 20:02:40 -0400</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>Computer Vision Framework 5 - Non-Iterative Superpixel Segmentation</title>
        <description>&lt;p&gt;This project is a continuation of the SLIC Superpixel segmentation project I &lt;a href=&quot;/academic/2019/11/07/vision3.html&quot;&gt;worked on earlier&lt;/a&gt;. &lt;!--description--&gt; SLIC is an inherently iterative algorithm which finds which pixels belong to which cluster centroids, moves the centroids, and checks again iteratively until convergence. This is a fairly memory intensive process due to the nature of every iteration has to keep track of every centroids relation to every pixel in range. It can be optimized for runtime by employing parallel calculations but the memory usage remains high regardless. &lt;a href=&quot;https://www.epfl.ch/labs/ivrl/research/snic-superpixels/&quot;&gt;SNIC&lt;/a&gt; improves upon this process in 2 main ways. It reduces memory usage by only checking nearby pixels at a given time instead of all in range, and eliminates the need for multiple iterations by online-updating the centroid locations as the algorithm runs. In order to do this, it uses a priority queue to keep tracking of the nearest-distance pixel to add to the cluster next. The only downside is due to this being mostly sequential, it cannot be parallelized as easily as SLIC. However if your main concern is memory usage or you do not have parallel computing capability on your platform, SNIC is a good alternative to SLIC.&lt;/p&gt;

&lt;p&gt;This project was mostly about comparing the performance and trade-offs of SLIC vs SNIC, so below are the comparisons I did:&lt;/p&gt;

&lt;p&gt;I chose to compare a few different implementations for a fair comparison, considering optimized and non-optimized versions of SLIC.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;JM-SLIC&lt;/b&gt; My original SLIC from project 3. A purely sequential implementation of simplified SLIC. Non-optimized.&lt;br /&gt;
&lt;b&gt;ZM-SLIC&lt;/b&gt; A provided vectorized implementation of simplified SLIC from project 3.&lt;br /&gt;
&lt;b&gt;SK-SLIC&lt;/b&gt; &lt;a href=&quot;https://scikit-image.org/&quot;&gt;scikit-image&lt;/a&gt; module's standard SLIC. Presented as an optimized version of the original SLIC white paper.&lt;br /&gt;
&lt;b&gt;JM-SNIC-8&lt;/b&gt; My python implementation of SNIC. Purely sequential operation. Non-optimized.&lt;br /&gt;
&lt;b&gt;JM-SNIC-4&lt;/b&gt; same as above, but with 4-connected neighbors instead of 8.&lt;/p&gt;

&lt;p&gt;Runtimes and memory usage with 150 super-pixel segments:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/cv5/comparisons.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;As we can see from the chart, of course the optimized and parallelized versions are much faster and efficient. However, comparing JM-SLIC to JM-SNIC, we can see the reduced memory and runtime advantages of SNIC over SLIC when not accounting for parallelism.  When using 4-connected neighbors instead of 8, my SNIC implementation even beats the memory usage of optimized SLIC.&lt;/p&gt;

&lt;p&gt;We must keep in mind that SNIC is a sequential algorithm by nature due to the use of the priority queue, and therefore cannot be parallelized nearly as much as SLIC, which can do cluster calculations concurrently or at least vectorized. 
Therefore, if reduced memory usage and complexity are goals, SNIC is a good choice. However, if you have the resources available to use, SLIC may end up a better choice for speed.&lt;/p&gt;

&lt;p&gt;Image output comparisons are below. We see that SNIC as implemented outputs comparable super pixels to the original SLIC. We also see that 4-connected SNIC outputs similar results to 8-connected, so we choose 4-connected as our default SNIC method as it is much faster and more memory efficient.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    JM-SLIC:&lt;br /&gt;
    &lt;img src=&quot;/assets/img/cv5/JM-SLIC.png&quot; /&gt;
    &lt;br /&gt;&lt;br /&gt;ZM-SLIC:&lt;br /&gt;
    &lt;img src=&quot;/assets/img/cv5/ZM-SLIC.png&quot; /&gt;
    &lt;br /&gt;&lt;br /&gt;SK-SLIC:&lt;br /&gt;
    &lt;img src=&quot;/assets/img/cv5/SK-SLIC.png&quot; /&gt;
    &lt;br /&gt;&lt;br /&gt;JM-SNIC-8:&lt;br /&gt;
    &lt;img src=&quot;/assets/img/cv5/JM-SNIC-8.png&quot; /&gt;
    &lt;br /&gt;&lt;br /&gt;JM-SNIC-4:&lt;br /&gt;
    &lt;img src=&quot;/assets/img/cv5/JM-SNIC-4.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 17 Dec 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/academic/2019/12/17/vision5.html</link>
        <guid isPermaLink="true">http://localhost:4000/academic/2019/12/17/vision5.html</guid>
        
        
        <category>Academic</category>
        
      </item>
    
      <item>
        <title>SQL database project - Extended multi-feature query processing engine</title>
        <description>&lt;p&gt;&lt;strong&gt;The Problem with Standard SQL:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Building and maintaining multi-dimensional queries is a challenge for multiple reasons. The language does not allow succinct representation of simple concepts such as Data pivots, aggregate functions over multiple groups of the same table, and hierarchical data comparisons. In order to represent most multi-dimensional queries, we must resort to a complicated and costly series of join operations across many different relational algebra operators and multiple sub-queries. Besides the extra confusion of how to actually build the query, This makes it difficult for a query engine to optimize without costly analysis, which is not feasible for ad-hoc queries.
&lt;!--description--&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Solution: Extended SQL&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Extended SQL and Multi Feature Queries were originally developed by Chatziantoniou et al across two publications: &lt;a href=&quot;https://www.semanticscholar.org/paper/Querying-Multiple-Features-of-Groups-in-Relational-Chatziantoniou-Ross/ace30b5576f6c5f57bd9cffd8c738e47c43b00c1&quot;&gt;Querying Multiple Features of Groups in Relational Databases&lt;/a&gt; and &lt;a href=&quot;https://www.semanticscholar.org/paper/Evaluation-of-ad-hoc-OLAP%3A-in-place-computation-Chatziantoniou/f7da51d6ce41170b88e2631a7060411a4881ea7b&quot;&gt;Evaluation of ad hoc OLAP: in-place computation&lt;/a&gt;. The papers describe a syntax that allows succinct expression of the types of queries queries mentioned above. This benefits us in a number of ways:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Easier for humans to understand and formulate: Less likely for bugs to occur and less difficult to maintain&lt;/li&gt;
  &lt;li&gt;Fewer costly join operations needed from the beginning: Leading to faster processing even before further optimization&lt;/li&gt;
  &lt;li&gt;More feasible to optimize: With the query represented by grouping variables and the Phi relational algebra operator, it does not require as much deep analysis to optimize the query in order to improve the processing time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;My Implementation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I created both an MF and EMF based query processing engine that takes queries constructed with the Phi operator (see details in the above publications) and generates python code that executes this query. 
During execution, the MF table is a series of nested default dictionaries.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1st level: Scan # (table scan pass)&lt;/li&gt;
  &lt;li&gt;2nd level: grouping attributes&lt;/li&gt;
  &lt;li&gt;3rd level: attribute we are aggregating&lt;/li&gt;
  &lt;li&gt;4th level: aggregate function
    &lt;font size=&quot;2&quot;&gt;(this is a Custom defaultdict made especially for storing aggregates where default values for min, max, and avg are Null and all others are 0.)&lt;/font&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This allows us to not waste space or time on attributes or aggregate functions that are not used in this query, while still allowing fast access of any possible attributes or aggregates needed.&lt;/p&gt;

&lt;p&gt;At a high level, the basic steps are as follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Parse the query data&lt;/li&gt;
  &lt;li&gt;Inject code for each grouping variable matching given conditions - One table scan per grouping variable, maintain aggregates of each.&lt;/li&gt;
  &lt;li&gt;Inject code to Build the output, scanning through all found grouping attributes for valid rows&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Overall Engine logic:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/sql/workflow.svg&quot; width=&quot;5000&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;This was a fun and challenging project. It was the first time I created code that then created other code so it was interesting coming up with good ways to manage that. Here are some specific challenges I dealt with along the way:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Problem&lt;/th&gt;
      &lt;th&gt;Analysis&lt;/th&gt;
      &lt;th&gt;Solution&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;I originally tried to generate everything about the output code structure in the processing engine first&lt;/td&gt;
      &lt;td&gt;This led to a lot of syntax and logic issues while debugging that were tedious to identify and correct&lt;/td&gt;
      &lt;td&gt;Instead, I created my output code by hand, keeping in mind which parts needed to be modular, then worked backwards to make the engine output that code&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Expected outputs were missing on sparse data&lt;/td&gt;
      &lt;td&gt;Null results were not being detected correctly&lt;/td&gt;
      &lt;td&gt;Switched to using try/except for all condition checks to avoid special logic for detecting Null cases.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        <pubDate>Sat, 07 Dec 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/academic/2019/12/07/SQL.html</link>
        <guid isPermaLink="true">http://localhost:4000/academic/2019/12/07/SQL.html</guid>
        
        
        <category>Academic</category>
        
      </item>
    
      <item>
        <title>Computer Vision Framework 4 - Visual Recognition</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_vision#Recognition&quot;&gt;Visual Recognition&lt;/a&gt; is another broad category of image processing, which is simply the idea of identifying things in an image and typically labeling them as such. There is a large variety of things that computer vision can be used to recognized, but for this project we focus on categorizing an image based on colors in the image, as well as classifying pixels in a particular image.&lt;/p&gt;

&lt;!--description--&gt;

&lt;p&gt;&lt;strong&gt;Image Classification&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For Image classification, we used the concept of creating histograms of color intensities for each color channel for the whole image. We had 3 different categories to train for: &quot;coast&quot;, &quot;forest&quot;, and &quot;inside city&quot;. I tried two different approaches. First I created a list of histograms for each training image. Then use the label of the nearest neighbor to the test image for classification.
Next, I created an average histogram for each label based on the average of the training image. I actually got better accuracy and faster classification for this method.
I also tried various bin sizes. The best seems to be around 7-10 bins with the average model, but for individual there isn't a huge difference from 2-15 bins.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/cv4/bins_imclass.png&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;Here are the actual classification results on my average model with 8 bins per color channel, in red and green highlights with the training and test images for reference, as well as a accuracy summary below:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/cv4/best_classification_results.jpg&quot; /&gt;

&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Label&lt;/th&gt;
      &lt;th&gt;Accuracy&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;coast&lt;/td&gt;
      &lt;td&gt;1.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;forest&lt;/td&gt;
      &lt;td&gt;1.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;inside_city&lt;/td&gt;
      &lt;td&gt;0.75&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;OVERALL&lt;/td&gt;
      &lt;td&gt;0.92&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;The one image, &quot;insidecity_test2&quot;, lacks any significant amount of red, unlike the inside city training images, while it does have more grays and slight blues so it was misclassified as coast. We can see a downside of this classification method is that all structural information of the location of these colors is ignored. However it is very fast to calculate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pixel Classification&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For Pixel classification we start with 1 training image and 4 testing images. The task was to first manually find the given the ground truth of sky pixels in the training image(by manual image masking techniques), then use k-means to find 10 &quot;visual words&quot; each for both &quot;sky&quot; pixels and &quot;not sky&quot; pixels. These visual words are essentially just pixel clusters, found in a similar way as &quot;K-means color segmentation&quot; of our &lt;a href=&quot;/academic/2019/11/07/vision3.html&quot;&gt;previous framework&lt;/a&gt;. based on my past experience with slow sequential code when using k-means, I use as much vectorized numpy code as I could to keep it fast. Then I simply find the closest center for each pixel in a test image to classify. Here are the results:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    Training image:&lt;br /&gt;
    &lt;img src=&quot;/assets/img/cv4/sky_train.jpg&quot; /&gt;
    &lt;br /&gt;&lt;br /&gt;Results:&lt;br /&gt;
    &lt;img src=&quot;/assets/img/cv4/pixel_class_results.jpg&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;This method works fairly well overall, but of course fails whenever there are non-sky pixels that are a close color to the sky or clouds. So the bluish flowers in test3 and white lines on the road on test 2 are identified as sky as well. You can also see some of the darker sky pixels in test4 are misclassified due to the training image only having brighter skys. A couple of ways to improve this would be to include spacial information for where sky pixels typically are, or to add extra logic to identify sky boundaries, such as assuming the sky is the largest contiguous section of sky pixels and ignoring other smaller sections.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 03 Dec 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/academic/2019/12/03/vision4.html</link>
        <guid isPermaLink="true">http://localhost:4000/academic/2019/12/03/vision4.html</guid>
        
        
        <category>Academic</category>
        
      </item>
    
      <item>
        <title>Computer Vision Framework 3 - Image Segmentation</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Image_segmentation&quot;&gt;Image Segmentation&lt;/a&gt; is a very broad category of image processing, but the basic idea is to separate various elements of an image into segments of a particular kind. In this project I will try out segmenting some images by pixel colors using &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;k-means clustering&lt;/a&gt;, and by so called &quot;Superpixels&quot; using a simplified version of the &lt;a href=&quot;https://infoscience.epfl.ch/record/149300&quot;&gt;SLIC&lt;/a&gt; algorithm.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/cv3/SLIC_orig.png&quot; height=&quot;200&quot; /&gt;
    &lt;br /&gt;
&lt;font size=&quot;1&quot;&gt;Superpixel Segmentation example from &lt;a href=&quot;https://infoscience.epfl.ch/record/149300&quot;&gt;SLIC Superpixels&lt;/a&gt; (Radhakrishna &lt;i&gt;et al.&lt;/i&gt;)&lt;/font&gt;
&lt;/p&gt;

&lt;!--description--&gt;
&lt;p&gt;&lt;strong&gt;Pre-processing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This was my first vision project dealing with RGB color channels instead of grayscale, so I had to decide how to deal with the extra layer of information. I initially chose to treat each channel as its own matrix and do every operation independently. While this made the code easy to think through and ultimately &lt;i&gt;worked&lt;/i&gt;, it slowed things down immensely as it prevented vectorized code as well as introduced problems when converting back to output. I instead recommend keeping colors together as a multidimensional matrix when doing this.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;K-means color segmentation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The idea here was to find 10 RGB values that are as close as possible to the RGB values of 10 clusters of pixels. In k-means terms these 10 RGB values are the centroids, and we need to find the closest pixels to those centroids and assign them the centroids RGB values once k means has converged. We ignored pixel locations and only considered colors for our clusters, which creates an effect of essentially reducing the image to 10 possible colors, similar to techniques that can be used for image compression. Here are the results on a sample image:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/cv3/white-tower.png&quot; /&gt;
    &lt;br /&gt;output:&lt;br /&gt;
    &lt;img src=&quot;/assets/img/cv3/white-tower-km.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Simplified SLIC Superpixels&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The goal of superpixels is to find pixels with common characteristics and put them into groups that are localized by location. Once found, this can be used for further image analysis or classification techniques such as object detection or semantic labeling without the complexity of analyzing all characteristics of each pixel individually. Furthermore, a superpixel contains much more visual information to work with compared to individual pixels, so you still have the flexibility of extracting that information when needed. The actual SLIC algorithm is fairly complex, so we did a simplified version. We start by choosing centers at chosen intervals, 50 pixels in our case, and then finding centroids and clusters by using the 5d space of RGB channels (3 dimensions) and x,y location (2 dimensions) This keeps the clusters in a relatively localized area while still shaping around the color and intensity of the pixels within them. Below is the output on a sample image with black borders drawn between each superpixel:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/cv3/slic_simple.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;You can see that some superpixels have areas interspersed with neighboring superpixels. The full SLIC algorithm has some additional steps to prevent this, but this was enough to demonstrate the idea behind SLIC so we did not need to account for it in our simplified version.&lt;/p&gt;

&lt;p&gt;Most of the challenges in this program were in identifying the correct numpy math operations to deal with the multi-dimensional data. This involved a lot of trial and error while checking if I had used the correct operation, which was compounded by the fact that the runtime grew exponentially with image size. Due to this, I used a very small image to test with and made sure my IDE could read numpy matrices directly from debug mode. Here are a few specific issues I ran into:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Problem&lt;/th&gt;
      &lt;th&gt;Analysis&lt;/th&gt;
      &lt;th&gt;Solution&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Runtime was super long (hours)&lt;/td&gt;
      &lt;td&gt;Code was entirely sequential at first, which was easier to read and comprehend, but not optimal&lt;/td&gt;
      &lt;td&gt;Clustering operations are possible to do in parallel, or at least vectorized. Switched to vectorized code working on multi-dimensional arrays cut runtime by magnitudes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SNIC clusters were collapsing into top left section per iteration&lt;/td&gt;
      &lt;td&gt;Location metrics were supposed to be halved in magnitude when computing their distance, but this contributed to relative position to origin pixel 0,0 also being halved&lt;/td&gt;
      &lt;td&gt;Corrected iterations to maintain relative position and only halve the contribution to centroid distance.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 07 Nov 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/academic/2019/11/07/vision3.html</link>
        <guid isPermaLink="true">http://localhost:4000/academic/2019/11/07/vision3.html</guid>
        
        
        <category>Academic</category>
        
      </item>
    
      <item>
        <title>Cloud Systems project - Distributed Hash Table Network</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Chord_(peer-to-peer)&quot;&gt;Chord&lt;/a&gt; is a peer-to-peer (p2p) protocol designed to maintain a distributed hash table. It primarily works by having each node in a network maintain a partial set of key-values pairs as well as a list of distributed nodes (known as a finger table) at strategic distributions of the network. It also constantly checks and updates the status of the network to maintain connectivity across potentially unreliable p2p nodes.&lt;/p&gt;

&lt;p&gt;For our Cloud and Distributed Systems project, we needed to implement a Chord-based distributed network in Java that establishes and maintains connections using RESTful APIs as well as supports remote shell operation of other nodes via websockets.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/dht/Chord_network.png&quot; height=&quot;200&quot; /&gt;
    &lt;br /&gt;
&lt;font size=&quot;1&quot;&gt;Chord network example with finger nodes highlighted from &lt;a href=&quot;https://en.wikipedia.org/wiki/Chord_(peer-to-peer)#/media/File:Chord_network.png&quot;&gt;Wikipedia.org&lt;/a&gt;&lt;/font&gt;
&lt;/p&gt;
&lt;!--description--&gt;

&lt;p&gt;Our project went in three phases. For the first phase we focused on using REST to connect, coordinate, and maintain Chord DHT nodes. Here is an overview of the structure of phase 1:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/dht/structure.png&quot; height=&quot;200&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;To implement REST we used the &lt;a href=&quot;https://eclipse-ee4j.github.io/jersey/&quot;&gt;Jersey&lt;/a&gt; framework which provides full support for &lt;a href=&quot;https://en.wikipedia.org/wiki/Java_API_for_RESTful_Web_Services&quot;&gt;JAX-RS APIs&lt;/a&gt;. Each node uses an embedded &lt;a href=&quot;https://javaee.github.io/grizzly/&quot;&gt;Grizzly&lt;/a&gt; web server to listen for and send REST calls.&lt;/p&gt;

&lt;p&gt;Chord's underlying logic relies on the idea of each node maintaining &lt;a href=&quot;https://en.wikipedia.org/wiki/Chord_(peer-to-peer)#Finger_table&quot;&gt;finger tables&lt;/a&gt;. These tables contains lists of nodes that are varying powers of 2 away from the current node, which allows you to reach a target node within O(log N) hops from any given node.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/dht/finger_table.png&quot; height=&quot;200&quot; /&gt;
    &lt;br /&gt;
&lt;font size=&quot;1&quot;&gt;Visual representation of node distances that are maintained in a finger table&lt;/font&gt;
&lt;/p&gt;

&lt;p&gt;These tables are maintained by various GET and PUT operations from each node. while GET, PUT and DELETE are used to actually add, update, or remove key value pairs at any given node.&lt;/p&gt;

&lt;p&gt;For the 2nd phase we implemented Server-Sent Events (&lt;a href=&quot;https://docs.huihoo.com/jersey/2.24/sse.html&quot;&gt;SSE&lt;/a&gt;) with Jersey in order to let the CLI from one node register to listen for incoming pushed events such as new or updated key/value bindings from another node. We also had to make sure nodes could fully communicate across independent networks, which meant confirming IP address routing and port usage for all protocols.&lt;/p&gt;

&lt;p&gt;For phase 3, we added &lt;a href=&quot;https://docs.oracle.com/javaee/7/api/javax/websocket/package-summary.html&quot;&gt;Websockets&lt;/a&gt; to allow a local CLI shell to remotely control a remote CLI shell. The websocket essentially just intercepted data that was being passed between the local CLI and backbend process and instead sends it through the websocket. As an added challenge we needed to maintain a stack of existing websockets to enable us to remember which node we were on before establishing a new websocket remote shell.&lt;/p&gt;

&lt;p&gt;This project was a great introduction to many core distributed systems principles. The majority of the issues with implementing it came down to becoming familiar with the APIs and frameworks that we had to use.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Problem&lt;/th&gt;
      &lt;th&gt;Analysis&lt;/th&gt;
      &lt;th&gt;Solution&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;RESTful PUT commands kept giving server errors, but only on certain requests&lt;/td&gt;
      &lt;td&gt;PUT requires an entity to be included with an explicit data type, both server and client must agree on the data format&lt;/td&gt;
      &lt;td&gt;made sure to explicitly designate what data formats were being sent and received for PUT commands. Also made sure there was always an entity included in the command&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Websocket remote control was losing context for local shell after closing all remote connections&lt;/td&gt;
      &lt;td&gt;Connection closing logic was killing entire shell when there were no more valid connections&lt;/td&gt;
      &lt;td&gt;Added extra CLI logic to handle return to local shell and trying to kill connection of a local shell&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        <pubDate>Sat, 26 Oct 2019 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/academic/2019/10/26/DHT.html</link>
        <guid isPermaLink="true">http://localhost:4000/academic/2019/10/26/DHT.html</guid>
        
        
        <category>Academic</category>
        
      </item>
    
      <item>
        <title>Computer Vision Framework 2 - Line Detection</title>
        <description>&lt;p&gt;Line Detection in image processing is the process of extracting structural information from an image in order to identify features in the image that correspond to lines. This can help with identifying movement or perspective changes as well as help to overlay emphasis of lines such as the line of scrimmage and First Down marker in a football broadcast:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/cv2/nfl_line.png&quot; width=&quot;450&quot; /&gt;
&lt;/p&gt;
&lt;!--description--&gt;
&lt;p&gt;I use two different algorithms for detecting lines for this project. The first is &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_sample_consensus&quot;&gt;RANSAC&lt;/a&gt; which is an iterative algorithm that is best for datasets with moderate matching parameters and is robust to noise and outliers as long as you have enough inliers to compensate. The second is &lt;a href=&quot;https://en.wikipedia.org/wiki/Hough_transform&quot;&gt;Hough Transform&lt;/a&gt;, which is not as good at dealing with uniform noise, but ok with random noise and can be used for multi-model recognition in a single run of the algorithm.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pre-processing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Before applying line detection, we need to identify points or &quot;features&quot; in the image that we can use to trace lines onto. There are a lot of different feature detection methods, but I use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hessian_affine_region_detector&quot;&gt;Hessian affine region detector&lt;/a&gt; which is great for detecting corners and strongly textured areas. It works by using the hessian determinant of each pixel and then choosing the maxima of these (above a threshold) as a feature. The hessian determinant formula is as follows: &lt;img src=&quot;/assets/img/cv2/hess_det.png&quot; width=&quot;150&quot; /&gt;
We can apply this by approximating the derivatives needed &lt;i&gt;I&lt;sub&gt;x&lt;/sub&gt;&lt;/i&gt; using Sobel filters on a gaussian blurred image and then applying non-maximum suppression.&lt;/p&gt;

&lt;p&gt;Here are the results of my hessian feature detecter, where feature pixels are in the green boxes:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/cv2/hess_box.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Method 1 - RANSAC&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;RANSAC is a fairly well documented algorithm. The basic approach is, given a set of sample points that contribute to a model, random choose a minimal set of points , create a model from these points, and find out how many other points from the dataset closely adhere to that model, also known as inliers. We repeat this process until we happen upon a model that has either some chosen minimal threshold of inliers or has done enough samples to statistically happen upon a good model. In order to avoid pre-calculating the number of minimum samples, we can use the apparent inlier/outlier ratio as the algorithm goes to automatically adapt the minimum sample count, which is the method I used.&lt;/p&gt;

&lt;p&gt;Here are the results on a sample image, using Pillow to draw the detected lines in red and the matching inliers in green.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/cv2/ransac.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Method 2 - Hough Transform&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hough Transforms have some other interesting applications compared to RANSAC, in that you cna use a model template to detect multiple different models within 1 run. However we were only searching for lines so this didn’t apply for us. The idea of Hough transform, is we find a coordinate space that covers all possible representations of the model we are matching, and then have every feature pixel in the image vote into “bins” for each model that would correspond with that pixel. Afterwords you choose the points in the coordinate space with the most votes as the strongest models. You do have to do some additional steps to avoid overlapping models. I simply applied non-maximum suppression of nearby bins. I also had to choose my coordinate space resolution (AKA voting bin size) in a way that balances algorithm run time with accuracy of model representation in the final image. I used the polar formula of a line model &lt;img src=&quot;/assets/img/cv2/polar_line.png&quot; width=&quot;150&quot; /&gt; with rho and theta as my coordinate space. This led to the below voting bins with the top 4 detected maxima in green boxes with vote counts:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/cv2/accum_best.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Below is the final image with detected lines shown in red and the feature pixels that voted for them in green. You can see the tree introduced enough semi-uniform noise such that the 4th strongest line detected corresponded with those feature points, but it was otherwise successful.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/cv2/hough.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;It was fun to get RANSAC working, and useful to see the effect of the adaptive element on run time. The visualization of the intermediate steps of hough really helped to fully understand how the algorithm was working. Overall this project went fairly smoothly and most time was spent on optimization and improving visualizations. Here are some challenges I ran into during the project:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Problem&lt;/th&gt;
      &lt;th&gt;Analysis&lt;/th&gt;
      &lt;th&gt;Solution&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Drawing lines/boxes to the image was not matching the correct location at all&lt;/td&gt;
      &lt;td&gt;Pillow image library uses reverse x,y coordinate system when drawing for legacy compatibility reasons&lt;/td&gt;
      &lt;td&gt;Simply reversed x,y in the command and it worked fine&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Drawn lines were off by a few pixels when overlayed on an image&lt;/td&gt;
      &lt;td&gt;The Gaussian Blur and Sobel filter steps involved trimming some pixels near the borders. The detected feature locations did not have these offsets included&lt;/td&gt;
      &lt;td&gt;Added offset auto-detection to code and added these offsets whenever drawing to image&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RANSAC was taking super long time to run&lt;/td&gt;
      &lt;td&gt;Adaptive algorithm was not working properly&lt;/td&gt;
      &lt;td&gt;Added debug status printing to find and correct the error&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 10 Oct 2019 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/academic/2019/10/10/vision2.html</link>
        <guid isPermaLink="true">http://localhost:4000/academic/2019/10/10/vision2.html</guid>
        
        
        <category>Academic</category>
        
      </item>
    
      <item>
        <title>Computer Vision Framework 1 - Edge Detection</title>
        <description>&lt;p&gt;In image processing, Edge Detection is the process of extracting structural information from an image in order to identify the edges of objects in the image in order to reduce the amount of visual information that needs to be processed. The results are interesting in themselves, but it is mostly useful for it's applications within higher level vision algorithms that can use detected edges to help identify moved or similar objects in different images or frames. Our first project in Computer Vision was to implement a simplified version of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Canny_edge_detector&quot;&gt;Canny Edge Detector&lt;/a&gt; on a given grayscale image. The process was straightforward and mostly designed to make you familiar with the tools and methods you need to use to start doing image processing.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://en.wikipedia.org/wiki/Canny_edge_detector#/media/File:Valve_monochrome_canny_(6).PNG&quot; title=&quot;Wikipedia image example&quot;&gt;
    &lt;img src=&quot;/assets/img/cv1/wiki_canny_ex.png&quot; alt=&quot;Wikipedia image example&quot; width=&quot;450&quot; /&gt;
  &lt;/a&gt;
  &lt;br /&gt;&lt;font size=&quot;1&quot;&gt;Canney Edge example from &lt;a href=&quot;https://en.wikipedia.org/wiki/Canny_edge_detector#Development_of_the_Canny_algorithm&quot;&gt;Wikipedia&lt;/a&gt;&lt;/font&gt;
&lt;/p&gt;
&lt;!--description--&gt;
&lt;p&gt;We consider our simplified edge detector to consist of the following steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Gaussian blur the image to reduce detection of edges on noise artifacts as well as change scope of edge detection&lt;/li&gt;
  &lt;li&gt;Compute the Gradient Magnitude of the image using Sobel filters, which we use as the basis of finding edges&lt;/li&gt;
  &lt;li&gt;Perform non-maximum suppression on the Gradient Magnitude, to form areas of large gradual change into a sharp defined edge&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Pre-processing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Before we do any of that however, we need to get our image into a format that we can manipulate within Python. For the purposes of our project, we were not allowed to use most existing image libraries except for the task of reading the image into an array of pixels (and writing this array back out to an image).
I chose to use &lt;a href=&quot;https://pypi.org/project/Pillow/&quot;&gt;Pillow&lt;/a&gt; to import our provided pgm images and stored them as &lt;a href=&quot;https://numpy.org/&quot;&gt;numpy&lt;/a&gt; matrices.
I did encounter some minor issues with the image framework, which I will cover later.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 1 - Gaussian Blur Filter&lt;/strong&gt;
Since this method of edge detection relies primarily on relative differences of pixel intensity for nearby pixels, image noise can greatly affect the output. Without Gaussian Blur, a single pixel of noise will be detected as a sharp edge. Furthermore, blurring an object can change the scope of object barriers to look at. As in, if you just want to see the edges of a large object, but not of the fine details of the objet, blurring it enough to remove those details will leave the large object edges. see how the edges of the details of the kangaroo disappear as we increase the area of the gaussian filter:&lt;/p&gt;

&lt;p&gt;sigma = 1:
&lt;img src=&quot;/assets/img/cv1/kang_s1.png&quot; alt=&quot;alt text&quot; title=&quot;kangaroo sigma=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;sigma = 3
&lt;img src=&quot;/assets/img/cv1/kang_s3.png&quot; alt=&quot;alt text&quot; title=&quot;kangaroo sigma = 3&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/img/cv1/Gaussian.png&quot; width=&quot;200&quot; /&gt;  
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
(The equation used to calculate pixel intensities for the Gaussian filter.)
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 2 - Gradient Magnitude&lt;/strong&gt;
Now that the image is blurred, we begin the task of finding the pixels where rapid changes happen in some axis. The theory being that any pixel intensity change must correspond with the edge of some object. There are a couple of different filters we could use for this effect, but we choose Sobel's filter. After applying this filter on the an image such as:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/cv1/plane.jpg&quot; alt=&quot;this one&quot; /&gt;&lt;/p&gt;

&lt;p&gt;we get this output:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/cv1/plane_sm.png&quot; alt=&quot;alt text&quot; title=&quot;plane sobel magnitude&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 3 - Non-maximum Suppression&lt;/strong&gt;
You may notice that the output of the last step does not give us clearly defined edges for the most part, and instead provides various blurred smudge representations of edges. This is because in the source image the intensity change actually happens over many pixels. The idea of non-maximum suppression is for any given area of pixels, we only consider the pixel that represents the maximum amount of change in gradient (the highest intensity from the previous step)  and suppress the rest fo the neighboring pixels. This allows clear 1-pixel edges to become visible and cleans up the remains of what is left. Since we don’t do the additional step of the original Canny algorithm of tracing lines that likely connect, I instead chose to keep the relative intensity values of the maximum pixels, so that you can infer where lines might connect.
Here is an example final output of the above plane:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/cv1/plane_good_mag.png&quot; alt=&quot;alt text&quot; title=&quot;plane final&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This project was a nice intro into Computer Vision algorithms, here are some if the issues that came up during development:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Problem&lt;/th&gt;
      &lt;th&gt;Analysis&lt;/th&gt;
      &lt;th&gt;Solution&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Images were all black or all white, despite pixel data in the array looking correct&lt;/td&gt;
      &lt;td&gt;PGM uses 8-bit ints for pixel data, data types were changing due to float operations and not scaling correctly when put back out to pgm format&lt;/td&gt;
      &lt;td&gt;Created a wrapper function on image output to warn of changed data type or bad data scale which also converted when needed. Also manually scaled and corrected data types where needed.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Gaussian blur output was noticeably higher intensity(brighter) than original image.&lt;/td&gt;
      &lt;td&gt;Normalization factor was off for certain pixel widths&lt;/td&gt;
      &lt;td&gt;Added an auto-scaling normalization factory that adjusted per filter size.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Final stage line intensities appeared to be random, not correlated with actual intensity of gradient magnitude &lt;img src=&quot;/assets/img/cv1/plane_bad_mag.png&quot; alt=&quot;alt text&quot; title=&quot;plane with bad magnitude &quot; /&gt;&lt;/td&gt;
      &lt;td&gt;Was using the gradient intensity of the axis with the highest value instead of the overall pixel gradient magnitude accounting for all axis leading to problems when there was large variation between axes&lt;/td&gt;
      &lt;td&gt;Once an edge is found by analyzing the maximum axis, use the total gradient magnitude as the intensity to represent the pixel&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 25 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/academic/2019/09/25/vision1.html</link>
        <guid isPermaLink="true">http://localhost:4000/academic/2019/09/25/vision1.html</guid>
        
        
        <category>Academic</category>
        
      </item>
    
      <item>
        <title>Data Structures project - Treap implementation in Java</title>
        <description>&lt;p&gt;A &lt;a href=&quot;https://en.wikipedia.org/wiki/Treap&quot;&gt;Treap&lt;/a&gt; is a pseudo auto-balanced Binary search Tree (BST), also referred to as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Self-balancing_binary_search_tree&quot;&gt;Self-balancing BST&lt;/a&gt; . It utilizes randomized heap priorities assigned to each new node added to a BST follow by heapify operations in order to attempt to keep the BST balanced. Since it relies on random operations, it is not quite as guaranteed to be balanced as other auto balancing BSTs such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Red%E2%80%93black_tree&quot;&gt;red-black trees&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/AVL_tree&quot;&gt;AVL trees&lt;/a&gt;, but over enough operations it will statistically perform just as well without all the extra complicated logic that these types of tree entail.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/ds/treap.png&quot; height=&quot;200&quot; /&gt;
    &lt;br /&gt;
&lt;font size=&quot;1&quot;&gt;Treap example from &lt;a href=&quot;https://www.geeksforgeeks.org/treap-a-randomized-binary-search-tree/&quot;&gt;GeeksforGeeks.org&lt;/a&gt;&lt;/font&gt;
&lt;/p&gt;
&lt;!--description--&gt;

&lt;p&gt;There are two basic concepts that make a Treap possible. In order to maintain heap priorities in a binary heap (which must have any parent node heap priority larger than its children), we must be able to push a node up the tree until it is higher than all its children.  In order to move a node up a tree without violating the BST properties, we use node rotation, where any 3 nodes (parent and 1-2 children) in a BST can be rotated in a way that maintains their BST order, but ends up with one of the children nodes now as the parent. Thus, whenever we add a node into a Treap, we first do a standard BST insert based on the given node data, keeping track of the path traversed to do so. Then we then rotate it up the path until the randomly assigned heap priority no longer violates heap rules.&lt;/p&gt;

&lt;p&gt;For my advanced data structures project, we had to implement to implement a Treap in Java from scratch in a way to take any comparable object (and therefore able to be stored in a BST) as the data object for each node.&lt;/p&gt;

&lt;p&gt;To do this I simply use &lt;a href=&quot;https://en.wikipedia.org/wiki/Generics_in_Java&quot;&gt;Java Generics&lt;/a&gt; to define the data object as &lt;code class=&quot;highlighter-rouge&quot;&gt;public class Treap&amp;lt;E extends Comparable&amp;lt;E&amp;gt;&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Furthermore, instead of the recursive version of adding and deleting a node, we needed to do an iterative version. To do this I maintained a stack of the path taken to add a node, and then passed this stack to a reheap function in order to restore/maintain the proper heap properties.&lt;/p&gt;

&lt;p&gt;This also complicated the rotate functions, as the parent node of the 3 nodes we are rotating around was not easily accessible. Thus, In order to avoid extra logic to find and alter these links, I just swapped data objects and heap priorities within the existing nodes, which maintained their parent and children links. Then moved around the other links as needed.&lt;/p&gt;

&lt;p&gt;Here is a visual representation demonstrating my RotateRight method. Here we need to rotate the A node up, since it has a higher priority (62) than the B node (53):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/img/ds/RotateRight.svg&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;You can see this works with or without any actual node in the C position and only modifies the links of nodes directly involved in the rotation operation.&lt;/p&gt;

&lt;p&gt;The rest of the implementation is fairly straightforward combinations of binary heap and BST operations.  For more details see the wikipedia or geeksforgeeks page on either.&lt;/p&gt;
</description>
        <pubDate>Fri, 26 Apr 2019 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/academic/2019/04/26/Treap.html</link>
        <guid isPermaLink="true">http://localhost:4000/academic/2019/04/26/Treap.html</guid>
        
        
        <category>Academic</category>
        
      </item>
    
      <item>
        <title>Home Arduino Project - Thermostat retrofit</title>
        <description>&lt;p&gt;Retrofitting an old Electric Baseboard heater with digital controls.
&lt;!--description--&gt;&lt;/p&gt;

&lt;p&gt;The goal was to essentially turn some decades-old electric baseboard heater with a simple on/off knob&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/old_and_busted.jpg&quot; alt=&quot;alt text&quot; title=&quot;old and busted&quot; /&gt;&lt;/p&gt;

&lt;p&gt;into a heater with modern digital controls that could actually be set to a temperature &lt;img src=&quot;/assets/img/newhotness.png&quot; alt=&quot;alt text&quot; title=&quot;new hotness&quot; /&gt;
 as well as be controlled without crawling to the corner of the room.
This was in a rental, so I really didn't want to rip out anything or spend a lot of money, but I had plenty of time to be creative so I turned to &lt;a href=&quot;https://www.arduino.cc/&quot;&gt;Arduino&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.arduino.cc/&quot;&gt;&lt;img src=&quot;/assets/img/720px-Arduino_Logo-svg.png&quot; width=&quot;50&quot; /&gt;&lt;/a&gt; is an open-source electronics platform based on easy-to-use hardware and software. It's intended for anyone making interactive projects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/img/Uno.jpg&quot;&gt;Here&lt;/a&gt; is a typical Arduino board with all the pinouts and features labeled:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;So why Arduino?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Cheap! Generic versions are &amp;lt; $10 per controller. Full versions that support the development are ~$20.&lt;/li&gt;
  &lt;li&gt;Popular! &lt;img src=&quot;/assets/img/ArduinoOSC.png&quot; width=&quot;200&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Lots of documentation, example, and public libraries&lt;/li&gt;
  &lt;li&gt;Works with many different hardware peripherals&lt;/li&gt;
  &lt;li&gt;Various starter kits available&lt;/li&gt;
  &lt;li&gt;Based on C++ which I am familiar with. Other languages also supported by open source efforts.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;How to start?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Design considerations&lt;/th&gt;
      &lt;th&gt;Solution&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Heating element is 250VAC&lt;/td&gt;
      &lt;td&gt;Use a high powered relay to control&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Quiet operation desired (in bedroom)&lt;/td&gt;
      &lt;td&gt;Use a solid state relay&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Heating element is on the ground in a corner&lt;/td&gt;
      &lt;td&gt;Temp sensor needs to be closer to human&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Ideal temp sensor and control location is opposite of a walkway and doorway to heater&lt;/td&gt;
      &lt;td&gt;Temp sensor and control needs to communicate wirelessly&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Initial Design&lt;/strong&gt;
&lt;img src=&quot;/assets/img/init-design.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So I started to put these elements together testing it along the way&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/wip1.jpg&quot; alt=&quot;alt text&quot; title=&quot;WIP&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is a test using a potentiometer as a temporary controller knob. The basic idea was working!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/wip2.jpg&quot; alt=&quot;alt text&quot; title=&quot;WIP&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I started with fairly basic code. &lt;a href=&quot;/assets/Nano_receive-active-low.ino&quot;&gt;Nano receiver code&lt;/a&gt;, &lt;a href=&quot;/assets/LCD-NRF_heater-digital.ino&quot;&gt;UNO control code&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All the Nano module needed to do was receive the On or Off state of heating element from the nRF24L01 radios and send out the signal to turn the relay on or off.
The UNO module would handle the controls and temperature sensing, and this is when the problems got more interesting:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Problem&lt;/th&gt;
      &lt;th&gt;Analysis&lt;/th&gt;
      &lt;th&gt;Solution&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Temp sensor readings were unstable. Especially when lights changed or buttons pressed.&lt;/td&gt;
      &lt;td&gt;Cheap temp sensor from kit used relative voltage in mV range to indicate temperature. Power source fluctuates in that range enough to affect readings.&lt;/td&gt;
      &lt;td&gt;Switch to $1 digital sensor with digital communication: DS18B20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Wireless communication unstable or freezing&lt;/td&gt;
      &lt;td&gt;Wireless uses a lot of energy! Voltage was dropping on signal spikes&lt;/td&gt;
      &lt;td&gt;Add capacitor to nano side and voltage regulator to Uno side&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LCD too bright at night&lt;/td&gt;
      &lt;td&gt;Need to find a way to dim the light&lt;/td&gt;
      &lt;td&gt;Implement PWM control for LCD backlight&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Dim backlight using more power than full bright and heating up??&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://forum.arduino.cc/index.php?topic=96747.0&quot;&gt;LCD Shield had bad wiring&lt;/a&gt;, PWM led to transistor breakdown&lt;/td&gt;
      &lt;td&gt;Rewire LCD backlight with resistor&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;Future work for this project&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;At this point I was considering the following improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3d Print a cover/enclosure for both sides  &lt;img src=&quot;/assets/img/cover.jpg&quot; width=&quot;100&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Create a web server or connect to cloud via wifi&lt;/li&gt;
  &lt;li&gt;Create mobile app for remote control&lt;/li&gt;
  &lt;li&gt;Add A/C controls for my window unit via infrared&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However…&lt;/p&gt;

&lt;p&gt;Then I moved into a new apartment with modern heating and a Nest built in. Thus ends the project.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jul 2016 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/automation/2016/07/03/thermometer.html</link>
        <guid isPermaLink="true">http://localhost:4000/automation/2016/07/03/thermometer.html</guid>
        
        
        <category>Automation</category>
        
      </item>
    
      <item>
        <title>NATCAR</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://www.ece.ucdavis.edu/natcar/&quot;&gt;NATCAR&lt;/a&gt; is a student design competition where teams design, build and race autonomous vehicles.&lt;!--description--&gt;&lt;/p&gt;

&lt;p&gt;When I competed, the vehicle was on a track marked by 1″-wide white tape with a wire carrying a 75 kHz sinusoidal signal (100 mA RMS) under the tape. My team of 3 designed a car that sensed magnetic field created by the wire and fed that information in to a microcontroller to autonomously control the car.&lt;/p&gt;

&lt;p&gt;We all worked on the hardware design, but my primary responsibility was to program the microcontroller. I had to develop a &lt;a href=&quot;https://en.wikipedia.org/wiki/PID_controller&quot;&gt;PID control loop&lt;/a&gt; for both acceleration and steering based on multiple sensor inputs, as well as build in extra logic to deal with complications of the track that the car had to navigate, which included sharp turns, crossed tracks, and penalties for straying too far. The code had to be optimized enough to run efficiently on a lightweight microcontroller, while still responsive enough to provide accurate real-time feedback to the drive motor and steering servo. Our team placed first at our college competition and 4th out of 29 in the national competition. This was a great practical application of coding and design that I found both challenging and rewarding.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/NATCAR.JPG&quot; alt=&quot;alt text&quot; title=&quot;Our car&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For those interested in the fine details of the design and troubleshooting process, continue on.&lt;/p&gt;

&lt;p&gt;The digital control system consists of a steering control system based on magnetic feedback sensors and a speed control system based on an optical interrupt sensor.
Before programming the processor to do any controlling, I had to calibrate the PWM signals for the steering servo as well as initialize and test the analog-to-digital converters. To calibrate the servo PWM signal I ran a simple test program to allow me to input a duty cycle and watch the results on the servo.
The first issue I dealt with was how often and when to sample what. For the purely proportional version of steering control, I had used a constant-polling while loop to check and adjust the steering at every possible CPU cycle. However, I recognized that the time required to calculate the steering term would be too inconsistent to use for derivative or integral control and decided instead to move to fixed rate polling. Since the fastest PWM signal that our servo reliably works on is 300 Hz, I decided to sample the magnetic sensors at a multiple of 13 above that, average those readings to reduce noise, and apply steering controls at 300 Hz. To do this, I implemented steering control within a real-time interrupt set for 3.9 KHz with a divider of 13.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;  &lt;span class=&quot;c1&quot;&gt;//init code to set the Adapt9S12E128 RTI to 3906Hz&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;RTICTL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0x20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 2^10 / 13 Set RTI divider for 300HZ , 4 Hz time base 0x7F, 0x20 for 3906Hz&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;cp&quot;&gt;#pragma interrupt_handler rti_handler
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rti_handler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;CRGFLG&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|=&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0x80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Clear the RTI Flag&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;//average readings per cycle to reduce noise&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sensorcount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// if new cycle, reset totals&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;r1sensort&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r2sensort&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1sensort&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2sensort&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; 
  &lt;span class=&quot;n&quot;&gt;r1sensort&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ATDDR9H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// accumulate&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;r2sensort&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ATDDR10H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;l1sensort&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ATDDR13H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;l2sensort&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ATDDR14H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sensorcount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//time divider&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sensorcount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//reset counter&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r1sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r1sensort&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// set averages&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r2sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r2sensort&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;l1sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1sensort&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;l2sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2sensort&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;//steering control here&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Once a sampling rate was established, I started trying more elaborate control methods and immediately fried our servo power supply when trying a new method. The steering control had attempted to send a PWM signal with a duty cycle entirely out of the servo's operation range. Because of this malfunction, I then implemented a range check on the duty cycle before assigning it to the PWM signal. This assures that the servo operates within a safe range and that instead of railing to either side and drawing too much current while stuck, it merely sits at the left or right extreme without attempting to push past it, saving much power.	To implement the steering derivative, I save the last averaged position value, compare it to the current one, and add this term to the proportional term scaled by a constant. I also check that the derivative is larger than the amount expected by noise before applying it.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dutycycle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;55&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// safety checks to prevent blown servo&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;dutycycle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;55&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dutycycle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;91&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;dutycycle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;91&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; 
  
  &lt;span class=&quot;n&quot;&gt;PWMDTY5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dutycycle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//apply value to actual PWM channel&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To implement the steering integral, I used a circular queue of size 75 to keep track of the moving average position offset. The array was intialized to 0 since that represents center position.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;c1&quot;&gt;//integration&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;positioni&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;positioni&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arraySize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// keep circular array index in bounds&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;movingTotal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;movingTotal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;positionarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;positioni&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//total = current total + current reading - oldest reading&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;positionarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;positioni&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//add current reading to array, overwriting oldest&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;positioni&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// increase array index&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;integral&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;movingTotal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arraySize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// update average	 &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To keep accurate track of the position of the car. We implemented 5 total sensors. 4 positional sensors plus 1 center sensor primarily to detect straightaways. We affixed these to a Lego-based support in order to make it easy to recover from crashes and adjust positions when needed.
&lt;img src=&quot;/assets/img/NATCAR-front.JPG&quot; alt=&quot;alt text&quot; title=&quot;Sensors are the 5 blue inductors with green wires&quot; /&gt;
I added some basic logic to translate if the wire has gone past the outer sensors and treated that as maximum offset of that side. It helped to recover from turns that were taken too sharply. (More on this later)&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r2sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r1Zero&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//4sensor version&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rsensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r2sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r1Offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//if outer sensor is strong enough, use it for readings&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r1sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r2sensor&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rsensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r1Offset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;250&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//if wire has passed all sensors, assume max value&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rsensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r1sensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l2sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1Zero&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//4s&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lsensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1Offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2sensor&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;lsensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1Offset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;250&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;lsensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1sensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;As well as logic to deal with detecting crosses in the track&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;c1&quot;&gt;//sharp cross detection, if both outer sensors are above threshold, must be at a cross, read inner sensors&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r2sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r1Zero&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1Zero&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//4 sensor version&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;lsensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1sensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rsensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r1sensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;and then scaled and combined the readings to use for the rest of the steering logic.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;n&quot;&gt;rsensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rsensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// scale down sensors to avoid overflow during evaluation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lsensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lsensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rsensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lsensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// set position according to right and left sensor values&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We found that merely keeping track of which side we lost the track on wasn't enough to recover from losing it. So I also added some logic to detect this and slow down until recovered:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;c1&quot;&gt;//overshoot section, check if lower than zero values&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;17&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1Zero&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r1sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;17&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r2sensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r1Zero&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;overshootSpeed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// set speed to slow down&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lastposition&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// right overshoot, error = rightmost&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//left overshoot, error = leftmost&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;//end overshoot&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;overshootSpeed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// not in overshoot, no slow down&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To allow easy adjustments during the competition, all constants are controlled by potentiometers set up on the extra ATD channels.
&lt;img src=&quot;/assets/img/NATCAR-pots.JPG&quot; alt=&quot;alt text&quot; title=&quot;There's a 5th pot behind the blurry capacitor there.&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;  &lt;span class=&quot;c1&quot;&gt;//read constants from inputs&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;targetpulses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ATDDR7H&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 0-128&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;intK&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ATDDR5H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 0-4  &lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;speedK&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ATDDR4H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 0-2&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;steeringDvK&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ATDDR6H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 0-8&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;steeringK&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ATDDR3H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 0-.5&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Although ATD conversions are somewhat slow, it is still magnitudes faster than our sampling rate and the ATD conversions are done in a separate module and therefore do not use main CPU cycles to calculate. The main disadvantages are the five extra cycles of assigning values and the extra power of having the ATD module fully active at all times. However, the power use is negligible and the assignments are at the end of the RTI cycle to avoid delaying time-critical calculations. I could have used multiple DIP switches set up on binary channels, but with 16 ATD channels available which would require much less custom hardware, I found it much easier to use ATD for setting constants.&lt;/p&gt;

&lt;p&gt;Speed control was a bit more difficult to program than the steering control section at first. Initially we used a non-feedback system which just sets the motor's PWM power line proportional to a potentiometer. However, this, of course, is a very unreliable speed system to due variable load on the motor at different speeds/acceleration. After setting up a pulse sensor on the axle, I programmed a speed control loop using the processor's pulse accumulator module.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;speedcalc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;195&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// do every 30 RTI cycles&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;speedcalc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;//reset time divider&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;currentpulse1count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;P1ACNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//use both accumulators, 1 for rising edge&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;currentpulse2count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;P2ACNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 1 for falling edge, increases resolution entirely in software&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pulses1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;currentpulse1count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lastpulse1count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pulses1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// if overflowed&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;pulses1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mh&quot;&gt;0xFFFF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// add offset&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pulses2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;currentpulse2count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lastpulse2count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pulses2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;pulses2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;  &lt;span class=&quot;mh&quot;&gt;0xFFFF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;	 
  &lt;span class=&quot;n&quot;&gt;pulses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pulses1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pulses2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//total pulses&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The loop runs every 1/10th of a second and calculates how many pulses passed in that time and compares it to how many it wanted to pass in that time, which was set by a potentiometer. It checks the difference between the target number of pulses for the past 1/10th of a second and the actual pulses, and also the change in the actual pulses since the last cycle. Both terms are scaled by constants and added together to calculate a correction term, which is added to the current duty cycle.
&lt;img src=&quot;/assets/img/NATCAR-speed.JPG&quot; alt=&quot;alt text&quot; title=&quot;The optical sensor tracked the rate that the slots on the wheel passed&quot; /&gt;
We also added some logic to detect when the car was on a large stretch of straight track and speed up in that situation using our center sensor.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ATDDR12H&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SPEEDTHRESHOLD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//if track is ahead and car is going straight&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;straightcount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// increase time known to be going straight&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;straightcount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// else, reset time&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;straightcount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// if going straight long enough, add straightaway speed&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;speedcorrection&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;targetpulses&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;speedK&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pulses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pulses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lastpulses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;overshootSpeed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// else add normal speed&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;speedcorrection&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;targetpulses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pulses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pulses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lastpulses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;overshootSpeed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For those that made it to the end, here's a nice macro shot of the microcontroller we used, the Adapt9S12E128. It utilizes a MC9S12E128 as the brains of the board.
&lt;img src=&quot;/assets/img/NATCAR-MC.JPG&quot; alt=&quot;alt text&quot; title=&quot;Thanks for reading!&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/NATCAR.c&quot;&gt;Full MC code available here&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 01 Jun 2009 09:06:26 -0400</pubDate>
        <link>http://localhost:4000/academic/2009/06/01/natcar.html</link>
        <guid isPermaLink="true">http://localhost:4000/academic/2009/06/01/natcar.html</guid>
        
        
        <category>Academic</category>
        
      </item>
    
  </channel>
</rss>
